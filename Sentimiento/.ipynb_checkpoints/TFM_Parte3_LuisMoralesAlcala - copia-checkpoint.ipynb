{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación del sentimiento en Tweets de Donald Trump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis descriptivo de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import import_ipynb\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['source','text','created_at','retweet_count','favorite_count','is_retweet','id_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Trump_Initial = pd.read_csv(\"./DataSheet_..trumptwitterarchive_com_archive.csv\",header=0,delimiter=';', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Trump_Initial.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Trump_Initial.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(df_Trump_Initial['Unnamed: 7'])\n",
    "del(df_Trump_Initial['Unnamed: 8'])\n",
    "del(df_Trump_Initial['Unnamed: 9'])\n",
    "del(df_Trump_Initial['Unnamed: 10'])\n",
    "df_Trump_Initial.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Trump_Initial_Date=df_Trump_Initial.created_at\n",
    "df_Trump_Initial_Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Trump_ToClean=pd.DataFrame({'mensaje': df_Trump_Initial.text, 'sentimiento': \"\"})\n",
    "df_Trump_ToClean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "import re as patternToDelete\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from unidecode import unidecode\n",
    "\n",
    "def deEmojify(inputString):\n",
    "    returnString = \"\"\n",
    "    for character in inputString:\n",
    "        try:\n",
    "            character.encode(\"ascii\")\n",
    "            returnString += character\n",
    "        except UnicodeEncodeError:\n",
    "            returnString += ''\n",
    "    return returnString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patternToDelete1 = r'@[A-Za-z0-9_]+'\n",
    "patternToDelete2 = r'https?://[^ ]+'\n",
    "patternToDelete3 = r'rt? +'\n",
    "patron = r'|'.join((patternToDelete1, patternToDelete2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "patronNegacion = patternToDelete.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "i=0\n",
    "def limpieza_tweets(mensaje):\n",
    "   \n",
    "    #patternToDelete.sub('https?://[A-Za-z0-9./]+','',mensaje)\n",
    "    out = BeautifulSoup(patternToDelete.sub('https?://[A-Za-z0-9./]+','',mensaje), 'lxml')\n",
    "    out_souped = out.get_text()\n",
    "    \n",
    "    try:\n",
    "        bom_borrado = out_souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_borrado = out_souped\n",
    "        \n",
    "    emoji_borrado = deEmojify(bom_borrado)\n",
    "    mensajeAux = patternToDelete.sub(patron, '', emoji_borrado)  \n",
    "    mensajeAux = patternToDelete.sub(www_pat, '', mensajeAux)\n",
    "    lmensajeAux_lower_case = mensajeAux.lower()\n",
    "    neg_handled = patronNegacion.sub(lambda x: negations_dic[x.group()], lmensajeAux_lower_case)\n",
    "    mensajeSoloLetras = patternToDelete.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    mensajeSoloLetras2 = patternToDelete.sub(patternToDelete3, \" \", mensajeSoloLetras)\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = [x for x  in tok.tokenize(mensajeSoloLetras2) if len(x) > 1]\n",
    "    return (\" \".join(words)).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print (\"Limpiando y parseando todos los tweets...\\n\")\n",
    "print(df_Trump_ToClean)\n",
    "tweetsLimpios = []\n",
    "i=0\n",
    "while i < len(df_Trump_ToClean):\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "        print (\"Tweets número %d de %d han sido procesados\" % ( i+1, len(df_Trump_ToClean) ))                                                                    \n",
    "    tweetsLimpios.append(limpieza_tweets(df_Trump_ToClean['mensaje'][i]))\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsLimpios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedo a la carga del modelo que mejores resultados me ha suministrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_t_tfidf=joblib.load('.\\Modelos Entrenados\\modelo_entrenado_t_tfidf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo=modelo_t_tfidf\n",
    "y_pred_Trump = modelo.predict(tweetsLimpios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Trump_Classified=pd.DataFrame({'mensaje': tweetsLimpios, 'sentimiento': y_pred_Trump})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Trump_Classified.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añado la columna fecha y reorganizo el dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Trump_Classified[\"fecha\"]=df_Trump_Initial_Date\n",
    "cols = df_Trump_Classified.columns.tolist()\n",
    "cols = ['mensaje','fecha','sentimiento']\n",
    "df_Trump_Classified= df_Trump_Classified[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Trump_Classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Trump_Classified = df_Trump_Classified.drop(df_Trump_Classified[df_Trump_Classified['mensaje']==''].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Trump_Classified.to_csv(\"trumpAUX.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Trump_Classified = pd.read_csv(\"trumpAUX.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neg_tweets = df_Trump_Classified[df_Trump_Classified.sentimiento == 0]\n",
    "neg_string = []\n",
    "for t in neg_tweets.mensaje:\n",
    "    neg_string.append(t)\n",
    "neg_string = pd.Series(neg_string).str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (16.0, 9.0)\n",
    "\n",
    "mask = np.array(Image.open(\"trump.jpg\"))\n",
    "colors = [\"#BF0A30\", \"#002868\"]\n",
    "cmap = LinearSegmentedColormap.from_list(\"mycmap\", colors)\n",
    "\n",
    "# Create WordCloud Object\n",
    "wc = WordCloud(background_color=\"white\",\n",
    "                 width=853, height=506, mask=mask, colormap=cmap)\n",
    "wc.generate(neg_string)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tweets = df_Trump_Classified[df_Trump_Classified.sentimiento == 1]\n",
    "pos_string = []\n",
    "for t in pos_tweets.mensaje:\n",
    "    pos_string.append(t)\n",
    "pos_string = pd.Series(pos_string).str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#BF0A30\", \"#002868\"]\n",
    "cmap = LinearSegmentedColormap.from_list(\"mycmap\", colors)\n",
    "\n",
    "# Create WordCloud Object\n",
    "wc = WordCloud(background_color=\"white\",\n",
    "                 width=853, height=506, mask=mask, colormap=cmap)\n",
    "wc.generate(pos_string)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#Convert a collection of text documents to a matrix of token counts\n",
    "cvec = CountVectorizer(stop_words='english')\n",
    "cvec.fit(df_Trump_Classified.mensaje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer ha extraido 16457 parabras del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_doc_matrix = cvec.transform(df_Trump_Classified[df_Trump_Classified.sentimiento == 0].mensaje)\n",
    "pos_doc_matrix = cvec.transform(df_Trump_Classified[df_Trump_Classified.sentimiento == 1].mensaje)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_tf = np.sum(neg_doc_matrix,axis=0)\n",
    "pos_tf = np.sum(pos_doc_matrix,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#squeeze function is used when we want to remove single-dimensional entries from the shape of an array.\n",
    "neg = np.squeeze(np.asarray(neg_tf))\n",
    "pos = np.squeeze(np.asarray(pos_tf))\n",
    "term_freq_df = pd.DataFrame([neg,pos],columns=cvec.get_feature_names()).transpose()\n",
    "type(term_freq_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = term_freq_df.columns.tolist()\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = term_freq_df.columns.tolist()\n",
    "names[names.index(0)] = 'negativo'\n",
    "names[names.index(1)] = 'positivo'\n",
    "term_freq_df.columns = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_df[\"Total\"]=term_freq_df.sum(axis=1)\n",
    "term_freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_df.to_csv('term_freq_df.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_df.sort_values(by='Total', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pos = np.arange(500)\n",
    "plt.figure(figsize=(10,8))\n",
    "s = 1\n",
    "expected_zipf = [term_freq_df.sort_values(by='Total', ascending=False)['Total'][0]/(i+1)**s for i in y_pos]\n",
    "plt.bar(y_pos, term_freq_df.sort_values(by='Total', ascending=False)['Total'][:500], align='center', alpha=0.5)\n",
    "plt.plot(y_pos, expected_zipf, color='r', linestyle='--',linewidth=2,alpha=0.5)\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 500 tokens in tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par la visualización de los tokens en los tweets de Trump voy a proceder a eliminar las stopword ya que me restan significado al análisis que quiero mostrar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cvec = CountVectorizer(stop_words='english',max_features=10000)\n",
    "cvec.fit(df_Trump_Classified.mensaje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_matrix = cvec.transform(df_Trump_Classified.mensaje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "neg_batches = np.linspace(0,len(df_Trump_Classified)/2,10).astype(int)\n",
    "i=0\n",
    "neg_tf = []\n",
    "while i < len(neg_batches)-1:\n",
    "    batch_result = np.sum(document_matrix[neg_batches[i]:neg_batches[i+1]].toarray(),axis=0)\n",
    "    neg_tf.append(batch_result)\n",
    "    print (neg_batches[i+1],\"entries' term freuquency calculated\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pos_batches = np.linspace(len(df_Trump_Classified)/2,len(df_Trump_Classified),10).astype(int)\n",
    "i=0\n",
    "pos_tf = []\n",
    "while i < len(pos_batches)-1:\n",
    "    batch_result = np.sum(document_matrix[pos_batches[i]:pos_batches[i+1]].toarray(),axis=0)\n",
    "    pos_tf.append(batch_result)\n",
    "    print (pos_batches[i+1],\"entries' term freuquency calculated\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = np.sum(neg_tf,axis=0)\n",
    "pos = np.sum(pos_tf,axis=0)\n",
    "term_freq_df2 = pd.DataFrame([neg,pos],columns=cvec.get_feature_names()).transpose()\n",
    "term_freq_df2.columns = ['negativo', 'positivo']\n",
    "term_freq_df2['Total'] = term_freq_df2['negativo'] + term_freq_df['positivo']\n",
    "term_freq_df2.sort_values(by='Total', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las 40 palabras más negativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pos = np.arange(40)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.bar(y_pos, term_freq_df.sort_values(by='negativo', ascending=False)['negativo'][:40], align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, term_freq_df.sort_values(by='negativo', ascending=False)['negativo'][:40].index,rotation='vertical')\n",
    "plt.ylabel('Frequencia')\n",
    "plt.xlabel('Los 40 tokens más negativos')\n",
    "plt.title('Los 40 tokens más negativos en tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pos = np.arange(40)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.bar(y_pos, term_freq_df.sort_values(by='positivo', ascending=False)['positivo'][:40], align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, term_freq_df.sort_values(by='positivo', ascending=False)['positivo'][:40].index,rotation='vertical')\n",
    "plt.ylabel('Frequencia')\n",
    "plt.xlabel('Los 40 tokens más positivos')\n",
    "plt.title('Los 40 tokens más positivos en tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Trump_Classified.to_csv('Trump_SA_For_LSTM.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Trump_Classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_Trump_Classified['sentimiento'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Porcentaje de tweets positivos: {}%\".format(df[1]*100/len(df_Trump_Classified)))\n",
    "print(\"Porcentaje de tweets negativos: {}%\".format(df[0]*100/len(df_Trump_Classified)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "labels=[\"Porcentaje de tweets positivos\", \"Porcentaje de tweets negativos\"]\n",
    "values = [df[1]*100/len(df_Trump_Classified), df[0]*100/len(df_Trump_Classified)]\n",
    "\n",
    "# Use `hole` to create a donut-like pie chart\n",
    "fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
