{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFM : Inferencia de noticias de Donald Trump en modelos de predicción de valores en Dow Jones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autor : Luis Morales Alcalá"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Análisis y limpieza de dataset de entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el desarrollo de este TFM voy a basarme en el dataset \"Sentiment140\" como dataset para el entrenamiento de mi modelo. Este dataset fue creado por la universidad de Stanford.\n",
    "\n",
    "A continuación presento un enlace donde se puede encontrar más información sobre el mismo:\n",
    "http://help.sentiment140.com/for-students/<br><br>\n",
    "El dataset se puede descargar del siguiente enlace.<br>\n",
    "http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
    "\n",
    "El formato del dataset es el siguiente:\n",
    "\n",
    "El archivo es un CSV con los emoticonos ya eliminados, dispone de 6 campos:\n",
    "\n",
    "0 - Polaridad del tweet (0 = negativo, 2 = neutral, 4 = positivo)<br>\n",
    "1 - el id del tweet (2087)<br>\n",
    "2 - Fecha del tweet (Sat May 16 23:58:44 UTC 2009)<br>\n",
    "3 - La query (lyx).Si no hay query, entonces el valor es NO_QUERY.<br>\n",
    "4 -El usuario que realizó el tweet (robotickilldozr)<br>\n",
    "5 - Texto del tweet (Lyx is cool)<br>\n",
    "\n",
    "\n",
    "Una vez entrenado y validado se procederá a realizar un análisis en profundidad del dataset del presidente Donald Trump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['sentimiento','id','fecha','query_string','user','mensaje']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inicial = pd.read_csv(\"./training.1600000.processed.noemoticon.csv\",header=None, names=cols,encoding='cp1252')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Análisis descriptivo de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inicial.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inicial.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inicial.sentimiento.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset contiene 1,6 millones de entradas que se encuentran balanceadas en dos tipos de sentimiento, negativo y positivo. En el dataset a pesar de que se indica que existe una clase neutral, como se puede observar no existe. Se destaca que no existen valores nulos por lo que en esta fase no será necesario aplicar ningún tipo de tratamiento del conjunto de datos al respecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceso a eliminar las categorías que no necesarias para la tarea que voy a acometer, para la tarea de clasificación de sentinmiento no es relevante información relativa a la fecha, id del mensaje, ni usuario ni si el mensaje a sido utilizado en una query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inicial.drop(['id','fecha','query_string','user'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inicial.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación paso a mostrar los 8 primeros mesajes con sentimiento negativo y positivo.\n",
    "\n",
    "Negativo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inicial[df_inicial.sentimiento==0].head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inicial[df_inicial.sentimiento==4].head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inicial[df_inicial.sentimiento == 4].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que el dataset presenta perfectamente divididos los mensajes clasificados como positivos y los negativos por del 0 --> 799999 los negativos y el resto los positivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inicial['sentimiento'] = df_inicial['sentimiento'].map({0: 0, 4: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inicial.sentimiento.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(df_inicial.sentimiento.value_counts(), labels=[\"Sentimiento negativo\", \"Sentimiento positivo\"], autopct=\"%0.1f %%\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voy a proceder a realizar un análisis del tamaño de la columna mensaje para verificar la diversidad de la muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inicial['longitud'] = [len(t) for t in df_inicial.mensaje]\n",
    "df_inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"El mensaje con mayor longitud es de: \"+str(df_inicial['longitud'].max())+\" caracteres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "plt.boxplot(df_inicial.longitud)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Preprocesamiento de los mensajes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuanción se presenta una muestra de ejemplo del tratamiento que tendrá que ser realizado al conjunto de los mensajes del dataset, para ello he identificado varios ejemplos de mensajes que presentan ciertas características específicas que son necesarias eliminar en esta fase de preprocesamiento para que posteriormente el modelo sea capaz de realizar unaclasificación lo más precisa posible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tratamiento de las menciones @"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tipo de funcionalidad de la red social permite que otro usuario mencione y ofrezca información relativa al mensaje original, por lo que no aporta valor en la construciión del modelo, al disponer ya del mensaje original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inicial.mensaje[600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re as patternToDelete\n",
    "patternToDelete.sub(r'@[A-Za-z0-9]+','',df_inicial.mensaje[600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tratamiento de código Html existente en los mensajes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedo a decodificar el texto html existente para lo que me apoyo en la funcion BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inicial.mensaje[119]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "out = BeautifulSoup(df_inicial.mensaje[119], 'lxml')\n",
    "print (out.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar una vez realizada la conversión se eliminan los textos del tipo '&quot','&amp'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 limpieza de links en los mensajes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inicial.mensaje[170]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patternToDelete.sub('https?://[A-Za-z0-9./]+','',df_inicial.mensaje[170])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Eliminación de hashtags y números"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siguiendo la misma filosofia que en los puntos anteriores, procedo a eliminar símbolos que entiendo que no son necesarios para el análisis de sentimiento como es el caso de la #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inicial.mensaje[190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patternToDelete.sub(\"[^a-zA-Z]\", \" \",df_inicial.mensaje[190])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez mostradas las cuatro tareas anteriores en las que se realiza un tratamiento de limpieza de los datos, se procede a la creación de una función que será ejecutada sobre el conjunto total de los datos.\n",
    "Esta función realizará las siguientes tareas de limpieza de los mensajes.\n",
    "\n",
    "1.Souping.<br>\n",
    "2.Eliminación del BOM.<br>\n",
    "3.Eliminación de direcciones http y www así como eliminación del ID de Twitter.<br>\n",
    "4.Conversión en minúsculas.<br>\n",
    "5.Manejo de las contracciones en las negaciones.<br>\n",
    "6.Eliminación de números y caracteres especiales.<br>\n",
    "7.Join y tokenización para eliminación de espacios en blanco.<br>\n",
    "8.Eliminación de emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordPunctTokenizer, esta función tokeniza una secuencia de caraceres alfanuméricos y no alfanuméricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "def give_emoji_free_text(text):\n",
    "    allchars = [str for str in text.decode('utf-8')]\n",
    "    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
    "    clean_text = ' '.join([str for str in text.decode('utf-8').split() if not any(i in str for i in emoji_list)])\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patternToDelete1 = r'@[A-Za-z0-9_]+'\n",
    "patternToDelete2 = r'https?://[^ ]+'\n",
    "patron = r'|'.join((patternToDelete1, patternToDelete2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "patronNegacion = patternToDelete.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def limpieza_tweets(mensaje):\n",
    "    out = BeautifulSoup(mensaje, 'lxml')\n",
    "    out_souped = out.get_text()\n",
    "    try:\n",
    "        bom_borrado = out_souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_borrado = out_souped\n",
    "    emoji_borrado = give_emoji_free_text(bom_borrado.encode('utf8'))\n",
    "    mensajeAux = patternToDelete.sub(patron, '', emoji_borrado)  \n",
    "    mensajeAux = patternToDelete.sub(www_pat, '', mensajeAux)\n",
    "    lmensajeAux_lower_case = mensajeAux.lower()\n",
    "    neg_handled = patronNegacion.sub(lambda x: negations_dic[x.group()], lmensajeAux_lower_case)\n",
    "    mensajeSoloLetras = patternToDelete.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = [x for x  in tok.tokenize(mensajeSoloLetras) if len(x) > 1]\n",
    "    return (\" \".join(words)).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print (\"Limpiando y parseando todos los tweets...\\n\")\n",
    "tweetsLimpios = []\n",
    "i=0\n",
    "while i < len(df_inicial):\n",
    "    if( (i+1)%100000 == 0 ):\n",
    "        print (\"Tweets número %d de %d han sido procesados\" % ( i+1, len(df_inicial) ))                                                                    \n",
    "    tweetsLimpios.append(limpieza_tweets(df_inicial['mensaje'][i]))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizo un análisis de los datos procesados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedo a guardar los mensajes limpios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsLimpiosDF = pd.DataFrame(tweetsLimpios,columns=['mensaje'])\n",
    "tweetsLimpiosDF['sentimiento'] = df_inicial.sentimiento\n",
    "tweetsLimpiosDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsLimpiosDF.to_csv('tweetsLimpios.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El tamaño de este nuevo csv una vez se ha aplicado la limpieza de los mensajes es de la mitad del tamaño original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsLimpiosDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
